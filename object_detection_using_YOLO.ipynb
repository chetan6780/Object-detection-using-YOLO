{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Sparks Foundation - Computer Vision & IOT Internship\n",
    "# Name: Chetan Ramesh Patil\n",
    "# Task 1: Object Detection (GRIPAPR21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When it comes to deep learning-based object detection, there are three primary object detectors you’ll encounter:\n",
    "\n",
    "1. R-CNN and their variants, including the original R-CNN, Fast R- CNN, and Faster R-CNN\n",
    "2. Single Shot Detector (SSDs)\n",
    "3. YOLO (You Only Look Once)\n",
    "\n",
    "While R-CNNs tend to very accurate, the biggest problem with the R-CNN family of networks is their speed — they were incredibly slow, obtaining only 5 FPS on a GPU. To help increase the speed of deep learning-based object detectors, both Single Shot Detectors (SSDs) and YOLO use a one-stage detector strategy.These algorithms treat object detection as a regression problem, taking a given input image and simultaneously learning bounding box coordinates and corresponding class label probabilities.In general, single-stage detectors tend to be less accurate than two-stage detectors but are significantly faster. YOLO is a great example of a single stage detector.\n",
    "\n",
    "I have tried both SSD and YOLO and i got better results with YOLO therefore below is the implimentaion of ***Object detection using YOLO***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will write paths for weight and configuration files and video path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = r'C:\\Users\\Admin\\Desktop\\TSF\\TASKS\\1\\object-detection-using-YOLO\\yolov3.weights'\n",
    "cfg_path = r'C:\\Users\\Admin\\Desktop\\TSF\\TASKS\\1\\object-detection-using-YOLO\\yolov3.cfg'\n",
    "video_path= r'C:\\Users\\Admin\\Desktop\\TSF\\TASKS\\1\\object-detection-using-YOLO\\media\\vid2.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating the list of names from the coco dataset and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "classes = []\n",
    "with open('coco.names', 'r') as f:\n",
    "    classes = f.read().splitlines()\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNetFromDarknet(cfg_path, weight_path)\n",
    "\n",
    "# capturing the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# setting up the confidence threshold and nms threshold\n",
    "confidence_threshold = 0.5\n",
    "nms_threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variables used are :\n",
    "1. **boxes**: Our bounding boxes around the object.\n",
    "2. **confidences** : The confidence value that YOLO assigns to an object. Lower confidence values indicate that the object might not be what the network thinks it is. Remember from our command line arguments above that we’ll filter out objects that don’t meet the 0.5 threshold.\n",
    "3. **class_ids**: The detected object’s class label.\n",
    "\n",
    "### Below code run continuesly untill we press Escape key to terminate it the process is described as followed:\n",
    "* image is in BGR we need to convert it to RGB and we convert it to blob image\n",
    "* Then we set the input from blob to network and get output layer names and from these names we can obtain the layer outputs as list\n",
    "* layer outputs gets 85 parameters first 4 are (x,y,w,h) and 5 th is confidence and remaining 80 are object names\n",
    "\n",
    "### use of Non-Maximum Suppression\n",
    "* To remove the multiple bounding boxes which have been detected we use **Non-Maximum Suppression** method it suppresses weak, overlapping bounding boxes in result giving only string bounding boxes in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    _, img = cap.read()\n",
    "    height, width, _ = img.shape\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        img, 1/255, (416, 416), (0, 0, 0), swapRB=True, crop=False)\n",
    "    \n",
    "    net.setInput(blob)  \n",
    "    output_layers_names = net.getUnconnectedOutLayersNames() \n",
    "    layerOutputs = net.forward(output_layers_names)\n",
    "\n",
    "    # creating variables to store the data\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    # Here we will extract the data from output layer\n",
    "    for output in layerOutputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]         # store score of 80 classes which starts from 6th index\n",
    "            class_id = np.argmax(scores)   # we get maximum score location\n",
    "            confidence = scores[class_id]  # get the score of the class_id\n",
    "            \n",
    "            # if confidence is satisfying we extract the center location and width, height of detected image\n",
    "            if confidence > confidence_threshold: \n",
    "                center_x = int(detection[0]*width)\n",
    "                center_y = int(detection[1]*height)\n",
    "                w = int(detection[2]*width)\n",
    "                h = int(detection[3]*height)\n",
    "\n",
    "                # get the position of upper corners\n",
    "                x = int(center_x-w/2)\n",
    "                y = int(center_y-h/2)\n",
    "\n",
    "                # collecting all information\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append((float(confidence)))\n",
    "                class_ids.append(class_id)\n",
    "                \n",
    "    # se of Non-max supression\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, nms_threshold)\n",
    "\n",
    "    # Defining font and variable color\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX\n",
    "    colors = np.random.uniform(0, 255, size=(len(boxes), 3))\n",
    "\n",
    "    # loop through each of the object detected and extract the information\n",
    "    if len(indexes) != 0:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            confidence = str(round(confidences[i]*100, 2))\n",
    "            color = colors[i]\n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), color, 2)\n",
    "            # putText: (img, text, org, fontFace, fontScale, color, thickness)\n",
    "            cv2.putText(img, label.upper()+\" \"+confidence, (boxes[i][0], boxes[i][1]),font, .85, (0, 255, 0), 2)\n",
    "            \n",
    "    # Showing the output\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:  # esc key\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly we release the capture object and clear all the windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
